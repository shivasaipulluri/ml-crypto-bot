{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ea0162f-1af1-401b-91fa-05f165ba39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 18:28:34,009 - INFO - Starting model training pipeline.\n",
      "2025-01-01 18:28:34,025 - INFO - Fetching historical data for SHIB-USD from 2024-01-03T00:28:34.025262 to 2025-01-02T00:28:34.025262 with granularity 900.\n",
      "2025-01-01 18:28:34,025 - INFO - Requesting data from 2024-01-03T00:28:34.025262 to 2024-01-06T03:28:34.025262.\n",
      "2025-01-01 18:28:34,197 - INFO - Requesting data from 2024-01-06T03:28:34.025262 to 2024-01-09T06:28:34.025262.\n",
      "2025-01-01 18:28:34,699 - INFO - Requesting data from 2024-01-09T06:28:34.025262 to 2024-01-12T09:28:34.025262.\n",
      "2025-01-01 18:28:34,997 - INFO - Requesting data from 2024-01-12T09:28:34.025262 to 2024-01-15T12:28:34.025262.\n",
      "2025-01-01 18:28:35,122 - INFO - Requesting data from 2024-01-15T12:28:34.025262 to 2024-01-18T15:28:34.025262.\n",
      "2025-01-01 18:28:35,607 - INFO - Requesting data from 2024-01-18T15:28:34.025262 to 2024-01-21T18:28:34.025262.\n",
      "2025-01-01 18:28:36,124 - INFO - Requesting data from 2024-01-21T18:28:34.025262 to 2024-01-24T21:28:34.025262.\n",
      "2025-01-01 18:28:36,625 - INFO - Requesting data from 2024-01-24T21:28:34.025262 to 2024-01-28T00:28:34.025262.\n",
      "2025-01-01 18:28:36,797 - INFO - Requesting data from 2024-01-28T00:28:34.025262 to 2024-01-31T03:28:34.025262.\n",
      "2025-01-01 18:28:37,142 - INFO - Requesting data from 2024-01-31T03:28:34.025262 to 2024-02-03T06:28:34.025262.\n",
      "2025-01-01 18:28:37,455 - INFO - Requesting data from 2024-02-03T06:28:34.025262 to 2024-02-06T09:28:34.025262.\n",
      "2025-01-01 18:28:37,753 - INFO - Requesting data from 2024-02-06T09:28:34.025262 to 2024-02-09T12:28:34.025262.\n",
      "2025-01-01 18:28:37,910 - INFO - Requesting data from 2024-02-09T12:28:34.025262 to 2024-02-12T15:28:34.025262.\n",
      "2025-01-01 18:28:38,160 - INFO - Requesting data from 2024-02-12T15:28:34.025262 to 2024-02-15T18:28:34.025262.\n",
      "2025-01-01 18:28:38,686 - INFO - Requesting data from 2024-02-15T18:28:34.025262 to 2024-02-18T21:28:34.025262.\n",
      "2025-01-01 18:28:38,849 - INFO - Requesting data from 2024-02-18T21:28:34.025262 to 2024-02-22T00:28:34.025262.\n",
      "2025-01-01 18:28:39,397 - INFO - Requesting data from 2024-02-22T00:28:34.025262 to 2024-02-25T03:28:34.025262.\n",
      "2025-01-01 18:28:39,944 - INFO - Requesting data from 2024-02-25T03:28:34.025262 to 2024-02-28T06:28:34.025262.\n",
      "2025-01-01 18:28:40,117 - INFO - Requesting data from 2024-02-28T06:28:34.025262 to 2024-03-02T09:28:34.025262.\n",
      "2025-01-01 18:28:40,634 - INFO - Requesting data from 2024-03-02T09:28:34.025262 to 2024-03-05T12:28:34.025262.\n",
      "2025-01-01 18:28:40,932 - INFO - Requesting data from 2024-03-05T12:28:34.025262 to 2024-03-08T15:28:34.025262.\n",
      "2025-01-01 18:28:41,245 - INFO - Requesting data from 2024-03-08T15:28:34.025262 to 2024-03-11T18:28:34.025262.\n",
      "2025-01-01 18:28:41,371 - INFO - Requesting data from 2024-03-11T18:28:34.025262 to 2024-03-14T21:28:34.025262.\n",
      "2025-01-01 18:28:41,746 - INFO - Requesting data from 2024-03-14T21:28:34.025262 to 2024-03-18T00:28:34.025262.\n",
      "2025-01-01 18:28:41,982 - INFO - Requesting data from 2024-03-18T00:28:34.025262 to 2024-03-21T03:28:34.025262.\n",
      "2025-01-01 18:28:42,139 - INFO - Requesting data from 2024-03-21T03:28:34.025262 to 2024-03-24T06:28:34.025262.\n",
      "2025-01-01 18:28:42,672 - INFO - Requesting data from 2024-03-24T06:28:34.025262 to 2024-03-27T09:28:34.025262.\n",
      "2025-01-01 18:28:42,829 - INFO - Requesting data from 2024-03-27T09:28:34.025262 to 2024-03-30T12:28:34.025262.\n",
      "2025-01-01 18:28:43,283 - INFO - Requesting data from 2024-03-30T12:28:34.025262 to 2024-04-02T15:28:34.025262.\n",
      "2025-01-01 18:28:43,596 - INFO - Requesting data from 2024-04-02T15:28:34.025262 to 2024-04-05T18:28:34.025262.\n",
      "2025-01-01 18:28:43,753 - INFO - Requesting data from 2024-04-05T18:28:34.025262 to 2024-04-08T21:28:34.025262.\n",
      "2025-01-01 18:28:44,113 - INFO - Requesting data from 2024-04-08T21:28:34.025262 to 2024-04-12T00:28:34.025262.\n",
      "2025-01-01 18:28:44,286 - INFO - Requesting data from 2024-04-12T00:28:34.025262 to 2024-04-15T03:28:34.025262.\n",
      "2025-01-01 18:28:44,724 - INFO - Requesting data from 2024-04-15T03:28:34.025262 to 2024-04-18T06:28:34.025262.\n",
      "2025-01-01 18:28:45,023 - INFO - Requesting data from 2024-04-18T06:28:34.025262 to 2024-04-21T09:28:34.025262.\n",
      "2025-01-01 18:28:45,430 - INFO - Requesting data from 2024-04-21T09:28:34.025262 to 2024-04-24T12:28:34.025262.\n",
      "2025-01-01 18:28:45,586 - INFO - Requesting data from 2024-04-24T12:28:34.025262 to 2024-04-27T15:28:34.025262.\n",
      "2025-01-01 18:28:45,837 - INFO - Requesting data from 2024-04-27T15:28:34.025262 to 2024-04-30T18:28:34.025262.\n",
      "2025-01-01 18:28:46,462 - INFO - Requesting data from 2024-04-30T18:28:34.025262 to 2024-05-03T21:28:34.025262.\n",
      "2025-01-01 18:28:46,775 - INFO - Requesting data from 2024-05-03T21:28:34.025262 to 2024-05-07T00:28:34.025262.\n",
      "2025-01-01 18:28:46,932 - INFO - Requesting data from 2024-05-07T00:28:34.025262 to 2024-05-10T03:28:34.025262.\n",
      "2025-01-01 18:28:47,386 - INFO - Requesting data from 2024-05-10T03:28:34.025262 to 2024-05-13T06:28:34.025262.\n",
      "2025-01-01 18:28:47,542 - INFO - Requesting data from 2024-05-13T06:28:34.025262 to 2024-05-16T09:28:34.025262.\n",
      "2025-01-01 18:28:47,792 - INFO - Requesting data from 2024-05-16T09:28:34.025262 to 2024-05-19T12:28:34.025262.\n",
      "2025-01-01 18:28:47,934 - INFO - Requesting data from 2024-05-19T12:28:34.025262 to 2024-05-22T15:28:34.025262.\n",
      "2025-01-01 18:28:48,199 - INFO - Requesting data from 2024-05-22T15:28:34.025262 to 2024-05-25T18:28:34.025262.\n",
      "2025-01-01 18:28:48,356 - INFO - Requesting data from 2024-05-25T18:28:34.025262 to 2024-05-28T21:28:34.025262.\n",
      "2025-01-01 18:28:48,512 - INFO - Requesting data from 2024-05-28T21:28:34.025262 to 2024-06-01T00:28:34.025262.\n",
      "2025-01-01 18:28:48,669 - INFO - Requesting data from 2024-06-01T00:28:34.025262 to 2024-06-04T03:28:34.025262.\n",
      "2025-01-01 18:28:49,029 - INFO - Requesting data from 2024-06-04T03:28:34.025262 to 2024-06-07T06:28:34.025262.\n",
      "2025-01-01 18:28:49,330 - INFO - Requesting data from 2024-06-07T06:28:34.025262 to 2024-06-10T09:28:34.025262.\n",
      "2025-01-01 18:28:49,639 - INFO - Requesting data from 2024-06-10T09:28:34.025262 to 2024-06-13T12:28:34.025262.\n",
      "2025-01-01 18:28:49,937 - INFO - Requesting data from 2024-06-13T12:28:34.025262 to 2024-06-16T15:28:34.025262.\n",
      "2025-01-01 18:28:50,360 - INFO - Requesting data from 2024-06-16T15:28:34.025262 to 2024-06-19T18:28:34.025262.\n",
      "2025-01-01 18:28:50,657 - INFO - Requesting data from 2024-06-19T18:28:34.025262 to 2024-06-22T21:28:34.025262.\n",
      "2025-01-01 18:28:51,175 - INFO - Requesting data from 2024-06-22T21:28:34.025262 to 2024-06-26T00:28:34.025262.\n",
      "2025-01-01 18:28:51,691 - INFO - Requesting data from 2024-06-26T00:28:34.025262 to 2024-06-29T03:28:34.025262.\n",
      "2025-01-01 18:28:51,993 - INFO - Requesting data from 2024-06-29T03:28:34.025262 to 2024-07-02T06:28:34.025262.\n",
      "2025-01-01 18:28:52,508 - INFO - Requesting data from 2024-07-02T06:28:34.025262 to 2024-07-05T09:28:34.025262.\n",
      "2025-01-01 18:28:52,712 - INFO - Requesting data from 2024-07-05T09:28:34.025262 to 2024-07-08T12:28:34.025262.\n",
      "2025-01-01 18:28:53,213 - INFO - Requesting data from 2024-07-08T12:28:34.025262 to 2024-07-11T15:28:34.025262.\n",
      "2025-01-01 18:28:53,370 - INFO - Requesting data from 2024-07-11T15:28:34.025262 to 2024-07-14T18:28:34.025262.\n",
      "2025-01-01 18:28:53,604 - INFO - Requesting data from 2024-07-14T18:28:34.025262 to 2024-07-17T21:28:34.025262.\n",
      "2025-01-01 18:28:53,934 - INFO - Requesting data from 2024-07-17T21:28:34.025262 to 2024-07-21T00:28:34.025262.\n",
      "2025-01-01 18:28:54,091 - INFO - Requesting data from 2024-07-21T00:28:34.025262 to 2024-07-24T03:28:34.025262.\n",
      "2025-01-01 18:28:54,234 - INFO - Requesting data from 2024-07-24T03:28:34.025262 to 2024-07-27T06:28:34.025262.\n",
      "2025-01-01 18:28:54,546 - INFO - Requesting data from 2024-07-27T06:28:34.025262 to 2024-07-30T09:28:34.025262.\n",
      "2025-01-01 18:28:54,891 - INFO - Requesting data from 2024-07-30T09:28:34.025262 to 2024-08-02T12:28:34.025262.\n",
      "2025-01-01 18:28:55,173 - INFO - Requesting data from 2024-08-02T12:28:34.025262 to 2024-08-05T15:28:34.025262.\n",
      "2025-01-01 18:28:55,471 - INFO - Requesting data from 2024-08-05T15:28:34.025262 to 2024-08-08T18:28:34.025262.\n",
      "2025-01-01 18:28:55,991 - INFO - Requesting data from 2024-08-08T18:28:34.025262 to 2024-08-11T21:28:34.025262.\n",
      "2025-01-01 18:28:56,506 - INFO - Requesting data from 2024-08-11T21:28:34.025262 to 2024-08-15T00:28:34.025262.\n",
      "2025-01-01 18:28:57,009 - INFO - Requesting data from 2024-08-15T00:28:34.025262 to 2024-08-18T03:28:34.025262.\n",
      "2025-01-01 18:28:57,525 - INFO - Requesting data from 2024-08-18T03:28:34.025262 to 2024-08-21T06:28:34.025262.\n",
      "2025-01-01 18:28:58,041 - INFO - Requesting data from 2024-08-21T06:28:34.025262 to 2024-08-24T09:28:34.025262.\n",
      "2025-01-01 18:28:58,541 - INFO - Requesting data from 2024-08-24T09:28:34.025262 to 2024-08-27T12:28:34.025262.\n",
      "2025-01-01 18:28:59,057 - INFO - Requesting data from 2024-08-27T12:28:34.025262 to 2024-08-30T15:28:34.025262.\n",
      "2025-01-01 18:28:59,370 - INFO - Requesting data from 2024-08-30T15:28:34.025262 to 2024-09-02T18:28:34.025262.\n",
      "2025-01-01 18:28:59,512 - INFO - Requesting data from 2024-09-02T18:28:34.025262 to 2024-09-05T21:28:34.025262.\n",
      "2025-01-01 18:28:59,972 - INFO - Requesting data from 2024-09-05T21:28:34.025262 to 2024-09-09T00:28:34.025262.\n",
      "2025-01-01 18:29:00,489 - INFO - Requesting data from 2024-09-09T00:28:34.025262 to 2024-09-12T03:28:34.025262.\n",
      "2025-01-01 18:29:00,803 - INFO - Requesting data from 2024-09-12T03:28:34.025262 to 2024-09-15T06:28:34.025262.\n",
      "2025-01-01 18:29:01,101 - INFO - Requesting data from 2024-09-15T06:28:34.025262 to 2024-09-18T09:28:34.025262.\n",
      "2025-01-01 18:29:01,620 - INFO - Requesting data from 2024-09-18T09:28:34.025262 to 2024-09-21T12:28:34.025262.\n",
      "2025-01-01 18:29:02,029 - INFO - Requesting data from 2024-09-21T12:28:34.025262 to 2024-09-24T15:28:34.025262.\n",
      "2025-01-01 18:29:02,544 - INFO - Requesting data from 2024-09-24T15:28:34.025262 to 2024-09-27T18:28:34.025262.\n",
      "2025-01-01 18:29:02,700 - INFO - Requesting data from 2024-09-27T18:28:34.025262 to 2024-09-30T21:28:34.025262.\n",
      "2025-01-01 18:29:03,045 - INFO - Requesting data from 2024-09-30T21:28:34.025262 to 2024-10-04T00:28:34.025262.\n",
      "2025-01-01 18:29:03,357 - INFO - Requesting data from 2024-10-04T00:28:34.025262 to 2024-10-07T03:28:34.025262.\n",
      "2025-01-01 18:29:03,514 - INFO - Requesting data from 2024-10-07T03:28:34.025262 to 2024-10-10T06:28:34.025262.\n",
      "2025-01-01 18:29:03,765 - INFO - Requesting data from 2024-10-10T06:28:34.025262 to 2024-10-13T09:28:34.025262.\n",
      "2025-01-01 18:29:04,287 - INFO - Requesting data from 2024-10-13T09:28:34.025262 to 2024-10-16T12:28:34.025262.\n",
      "2025-01-01 18:29:04,798 - INFO - Requesting data from 2024-10-16T12:28:34.025262 to 2024-10-19T15:28:34.025262.\n",
      "2025-01-01 18:29:05,330 - INFO - Requesting data from 2024-10-19T15:28:34.025262 to 2024-10-22T18:28:34.025262.\n",
      "2025-01-01 18:29:05,926 - INFO - Requesting data from 2024-10-22T18:28:34.025262 to 2024-10-25T21:28:34.025262.\n",
      "2025-01-01 18:29:06,431 - INFO - Requesting data from 2024-10-25T21:28:34.025262 to 2024-10-29T00:28:34.025262.\n",
      "2025-01-01 18:29:06,745 - INFO - Requesting data from 2024-10-29T00:28:34.025262 to 2024-11-01T03:28:34.025262.\n",
      "2025-01-01 18:29:07,261 - INFO - Requesting data from 2024-11-01T03:28:34.025262 to 2024-11-04T06:28:34.025262.\n",
      "2025-01-01 18:29:07,529 - INFO - Requesting data from 2024-11-04T06:28:34.025262 to 2024-11-07T09:28:34.025262.\n",
      "2025-01-01 18:29:07,780 - INFO - Requesting data from 2024-11-07T09:28:34.025262 to 2024-11-10T12:28:34.025262.\n",
      "2025-01-01 18:29:08,015 - INFO - Requesting data from 2024-11-10T12:28:34.025262 to 2024-11-13T15:28:34.025262.\n",
      "2025-01-01 18:29:08,374 - INFO - Requesting data from 2024-11-13T15:28:34.025262 to 2024-11-16T18:28:34.025262.\n",
      "2025-01-01 18:29:08,515 - INFO - Requesting data from 2024-11-16T18:28:34.025262 to 2024-11-19T21:28:34.025262.\n",
      "2025-01-01 18:29:08,890 - INFO - Requesting data from 2024-11-19T21:28:34.025262 to 2024-11-23T00:28:34.025262.\n",
      "2025-01-01 18:29:09,421 - INFO - Requesting data from 2024-11-23T00:28:34.025262 to 2024-11-26T03:28:34.025262.\n",
      "2025-01-01 18:29:09,765 - INFO - Requesting data from 2024-11-26T03:28:34.025262 to 2024-11-29T06:28:34.025262.\n",
      "2025-01-01 18:29:10,468 - INFO - Requesting data from 2024-11-29T06:28:34.025262 to 2024-12-02T09:28:34.025262.\n",
      "2025-01-01 18:29:10,827 - INFO - Requesting data from 2024-12-02T09:28:34.025262 to 2024-12-05T12:28:34.025262.\n",
      "2025-01-01 18:29:11,249 - INFO - Requesting data from 2024-12-05T12:28:34.025262 to 2024-12-08T15:28:34.025262.\n",
      "2025-01-01 18:29:11,547 - INFO - Requesting data from 2024-12-08T15:28:34.025262 to 2024-12-11T18:28:34.025262.\n",
      "2025-01-01 18:29:11,859 - INFO - Requesting data from 2024-12-11T18:28:34.025262 to 2024-12-14T21:28:34.025262.\n",
      "2025-01-01 18:29:12,376 - INFO - Requesting data from 2024-12-14T21:28:34.025262 to 2024-12-18T00:28:34.025262.\n",
      "2025-01-01 18:29:12,681 - INFO - Requesting data from 2024-12-18T00:28:34.025262 to 2024-12-21T03:28:34.025262.\n",
      "2025-01-01 18:29:13,191 - INFO - Requesting data from 2024-12-21T03:28:34.025262 to 2024-12-24T06:28:34.025262.\n",
      "2025-01-01 18:29:13,707 - INFO - Requesting data from 2024-12-24T06:28:34.025262 to 2024-12-27T09:28:34.025262.\n",
      "2025-01-01 18:29:13,848 - INFO - Requesting data from 2024-12-27T09:28:34.025262 to 2024-12-30T12:28:34.025262.\n",
      "2025-01-01 18:29:14,319 - INFO - Requesting data from 2024-12-30T12:28:34.025262 to 2025-01-02T00:28:34.025262.\n",
      "2025-01-01 18:29:14,979 - INFO - Historical data fetch complete. Total rows fetched: 35030.\n",
      "2025-01-01 18:29:14,995 - INFO - Starting feature engineering.\n",
      "2025-01-01 18:29:15,042 - INFO - Feature engineering complete. Final dataset shape: (35001, 8).\n",
      "2025-01-01 18:29:15,059 - INFO - Preparing data for LSTM.\n",
      "2025-01-01 18:29:15,121 - INFO - Data preparation complete. Total samples: 34971.\n",
      "[I 2025-01-01 18:29:15,277] A new study created in memory with name: no-name-5be99f20-bfdd-4c97-b045-06e5508a113f\n",
      "2025-01-01 18:29:15,277 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 18:29:15,371 - INFO - Building LSTM model with input shape (30, 8), units=90, dropout=0.3707912725439343.\n",
      "C:\\Users\\pullu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "2025-01-01 18:29:16,753 - INFO - LSTM model build complete.\n",
      "2025-01-01 18:35:54,403 - INFO - Validation loss: 0.0002525934251025319.\n",
      "[I 2025-01-01 18:35:54,403] Trial 0 finished with value: 0.0002525934251025319 and parameters: {'units': 90, 'dropout': 0.3707912725439343, 'learning_rate': 0.004878414033696183}. Best is trial 0 with value: 0.0002525934251025319.\n",
      "2025-01-01 18:35:54,419 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 18:35:54,513 - INFO - Building LSTM model with input shape (30, 8), units=88, dropout=0.2211717295980319.\n",
      "2025-01-01 18:35:54,701 - INFO - LSTM model build complete.\n",
      "2025-01-01 18:42:07,465 - INFO - Validation loss: 0.00023808029072824866.\n",
      "[I 2025-01-01 18:42:07,481] Trial 1 finished with value: 0.00023808029072824866 and parameters: {'units': 88, 'dropout': 0.2211717295980319, 'learning_rate': 0.00019790747076322947}. Best is trial 1 with value: 0.00023808029072824866.\n",
      "2025-01-01 18:42:07,481 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 18:42:07,591 - INFO - Building LSTM model with input shape (30, 8), units=90, dropout=0.10374316434709643.\n",
      "2025-01-01 18:42:07,795 - INFO - LSTM model build complete.\n",
      "2025-01-01 18:48:55,123 - INFO - Validation loss: 0.00017056186334230006.\n",
      "[I 2025-01-01 18:48:55,155] Trial 2 finished with value: 0.00017056186334230006 and parameters: {'units': 90, 'dropout': 0.10374316434709643, 'learning_rate': 0.002807783509566722}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 18:48:55,156 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 18:48:55,282 - INFO - Building LSTM model with input shape (30, 8), units=70, dropout=0.4000449010754209.\n",
      "2025-01-01 18:48:55,534 - INFO - LSTM model build complete.\n",
      "2025-01-01 18:54:20,039 - INFO - Validation loss: 0.0002353216550545767.\n",
      "[I 2025-01-01 18:54:20,054] Trial 3 finished with value: 0.0002353216550545767 and parameters: {'units': 70, 'dropout': 0.4000449010754209, 'learning_rate': 0.008288711227048365}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 18:54:20,054 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 18:54:20,132 - INFO - Building LSTM model with input shape (30, 8), units=76, dropout=0.25641366707498736.\n",
      "2025-01-01 18:54:20,321 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:00:19,005 - INFO - Validation loss: 0.0003766114532481879.\n",
      "[I 2025-01-01 19:00:19,021] Trial 4 finished with value: 0.0003766114532481879 and parameters: {'units': 76, 'dropout': 0.25641366707498736, 'learning_rate': 0.00011934286288641264}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 19:00:19,021 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:00:19,116 - INFO - Building LSTM model with input shape (30, 8), units=92, dropout=0.2364850344520856.\n",
      "2025-01-01 19:00:19,319 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:06:55,931 - INFO - Validation loss: 0.0002504642470739782.\n",
      "[I 2025-01-01 19:06:55,947] Trial 5 finished with value: 0.0002504642470739782 and parameters: {'units': 92, 'dropout': 0.2364850344520856, 'learning_rate': 0.00019592922729754782}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 19:06:55,947 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:06:56,042 - INFO - Building LSTM model with input shape (30, 8), units=81, dropout=0.4666887460563821.\n",
      "2025-01-01 19:06:56,230 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:13:30,934 - INFO - Validation loss: 0.00019999714277219027.\n",
      "[I 2025-01-01 19:13:30,965] Trial 6 finished with value: 0.00019999714277219027 and parameters: {'units': 81, 'dropout': 0.4666887460563821, 'learning_rate': 0.0015603311971472546}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 19:13:30,981 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:13:31,090 - INFO - Building LSTM model with input shape (30, 8), units=97, dropout=0.19908458864366324.\n",
      "2025-01-01 19:13:31,277 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:21:05,687 - INFO - Validation loss: 0.0003772717318497598.\n",
      "[I 2025-01-01 19:21:05,703] Trial 7 finished with value: 0.0003772717318497598 and parameters: {'units': 97, 'dropout': 0.19908458864366324, 'learning_rate': 0.0036900130963738154}. Best is trial 2 with value: 0.00017056186334230006.\n",
      "2025-01-01 19:21:05,718 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:21:05,812 - INFO - Building LSTM model with input shape (30, 8), units=42, dropout=0.19976347994366025.\n",
      "2025-01-01 19:21:06,000 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:25:13,081 - INFO - Validation loss: 0.00015799648826941848.\n",
      "[I 2025-01-01 19:25:13,096] Trial 8 finished with value: 0.00015799648826941848 and parameters: {'units': 42, 'dropout': 0.19976347994366025, 'learning_rate': 0.0017232962273117125}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:25:13,096 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:25:13,194 - INFO - Building LSTM model with input shape (30, 8), units=55, dropout=0.3655561817563452.\n",
      "2025-01-01 19:25:13,379 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:30:45,489 - INFO - Validation loss: 0.0002721288474276662.\n",
      "[I 2025-01-01 19:30:45,505] Trial 9 finished with value: 0.0002721288474276662 and parameters: {'units': 55, 'dropout': 0.3655561817563452, 'learning_rate': 0.00025773984883158433}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:30:45,505 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:30:45,692 - INFO - Building LSTM model with input shape (30, 8), units=35, dropout=0.12282922303993471.\n",
      "2025-01-01 19:30:45,880 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:34:41,803 - INFO - Validation loss: 0.00017671487876214087.\n",
      "[I 2025-01-01 19:34:41,818] Trial 10 finished with value: 0.00017671487876214087 and parameters: {'units': 35, 'dropout': 0.12282922303993471, 'learning_rate': 0.0008872279230443938}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:34:41,818 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:34:41,990 - INFO - Building LSTM model with input shape (30, 8), units=49, dropout=0.10323700325279118.\n",
      "2025-01-01 19:34:42,178 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:39:21,427 - INFO - Validation loss: 0.0001587290462339297.\n",
      "[I 2025-01-01 19:39:21,443] Trial 11 finished with value: 0.0001587290462339297 and parameters: {'units': 49, 'dropout': 0.10323700325279118, 'learning_rate': 0.0013440308800550691}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:39:21,443 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:39:21,616 - INFO - Building LSTM model with input shape (30, 8), units=51, dropout=0.16110279663662241.\n",
      "2025-01-01 19:39:21,787 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:44:05,786 - INFO - Validation loss: 0.00017118392861448228.\n",
      "[I 2025-01-01 19:44:05,802] Trial 12 finished with value: 0.00017118392861448228 and parameters: {'units': 51, 'dropout': 0.16110279663662241, 'learning_rate': 0.0007390687584972824}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:44:05,817 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:44:05,990 - INFO - Building LSTM model with input shape (30, 8), units=32, dropout=0.300315733027803.\n",
      "2025-01-01 19:44:06,162 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:47:45,288 - INFO - Validation loss: 0.000259168678894639.\n",
      "[I 2025-01-01 19:47:45,303] Trial 13 finished with value: 0.000259168678894639 and parameters: {'units': 32, 'dropout': 0.300315733027803, 'learning_rate': 0.0004915522811813806}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:47:45,320 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:47:45,507 - INFO - Building LSTM model with input shape (30, 8), units=46, dropout=0.161990521185848.\n",
      "2025-01-01 19:47:45,679 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:52:43,272 - INFO - Validation loss: 0.00018188278772868216.\n",
      "[I 2025-01-01 19:52:43,288] Trial 14 finished with value: 0.00018188278772868216 and parameters: {'units': 46, 'dropout': 0.161990521185848, 'learning_rate': 0.0016577270597569562}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:52:43,304 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:52:43,491 - INFO - Building LSTM model with input shape (30, 8), units=60, dropout=0.16801390719880233.\n",
      "2025-01-01 19:52:43,679 - INFO - LSTM model build complete.\n",
      "2025-01-01 19:57:39,912 - INFO - Validation loss: 0.0004794515552930534.\n",
      "[I 2025-01-01 19:57:39,927] Trial 15 finished with value: 0.0004794515552930534 and parameters: {'units': 60, 'dropout': 0.16801390719880233, 'learning_rate': 0.0018566390595197864}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 19:57:39,927 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 19:57:40,115 - INFO - Building LSTM model with input shape (30, 8), units=42, dropout=0.27270804517205904.\n",
      "2025-01-01 19:57:40,272 - INFO - LSTM model build complete.\n",
      "2025-01-01 20:01:46,765 - INFO - Validation loss: 0.00019394814444240183.\n",
      "[I 2025-01-01 20:01:46,781] Trial 16 finished with value: 0.00019394814444240183 and parameters: {'units': 42, 'dropout': 0.27270804517205904, 'learning_rate': 0.0004980855912493889}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 20:01:46,781 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 20:01:46,969 - INFO - Building LSTM model with input shape (30, 8), units=42, dropout=0.12917916102787513.\n",
      "2025-01-01 20:01:47,141 - INFO - LSTM model build complete.\n",
      "2025-01-01 20:05:53,612 - INFO - Validation loss: 0.00039068181649781764.\n",
      "[I 2025-01-01 20:05:53,628] Trial 17 finished with value: 0.00039068181649781764 and parameters: {'units': 42, 'dropout': 0.12917916102787513, 'learning_rate': 0.007864259911702502}. Best is trial 8 with value: 0.00015799648826941848.\n",
      "2025-01-01 20:05:53,628 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 20:05:53,878 - INFO - Building LSTM model with input shape (30, 8), units=64, dropout=0.18919924456831705.\n",
      "2025-01-01 20:05:54,082 - INFO - LSTM model build complete.\n",
      "2025-01-01 20:10:53,876 - INFO - Validation loss: 0.00015461433213204145.\n",
      "[I 2025-01-01 20:10:53,891] Trial 18 finished with value: 0.00015461433213204145 and parameters: {'units': 64, 'dropout': 0.18919924456831705, 'learning_rate': 0.0012269713853544965}. Best is trial 18 with value: 0.00015461433213204145.\n",
      "2025-01-01 20:10:53,891 - INFO - Starting hyperparameter tuning with Optuna.\n",
      "2025-01-01 20:10:54,047 - INFO - Building LSTM model with input shape (30, 8), units=66, dropout=0.30454558110663044.\n",
      "2025-01-01 20:10:54,236 - INFO - LSTM model build complete.\n",
      "2025-01-01 20:16:12,215 - INFO - Validation loss: 0.00020858853531535715.\n",
      "[I 2025-01-01 20:16:12,230] Trial 19 finished with value: 0.00020858853531535715 and parameters: {'units': 66, 'dropout': 0.30454558110663044, 'learning_rate': 0.0006089244113206699}. Best is trial 18 with value: 0.00015461433213204145.\n",
      "2025-01-01 20:16:12,230 - INFO - Best parameters: {'units': 64, 'dropout': 0.18919924456831705, 'learning_rate': 0.0012269713853544965}.\n",
      "2025-01-01 20:16:12,246 - INFO - Building LSTM model with input shape (30, 8), units=64, dropout=0.18919924456831705.\n",
      "2025-01-01 20:16:12,449 - INFO - LSTM model build complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 0.0044  \n",
      "Epoch 1: val_loss improved from inf to 0.00024, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 80ms/step - loss: 0.0044 - val_loss: 2.4187e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 7.8821e-04  \n",
      "Epoch 2: val_loss improved from 0.00024 to 0.00016, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 79ms/step - loss: 7.8807e-04 - val_loss: 1.5899e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 4.6787e-04  \n",
      "Epoch 3: val_loss did not improve from 0.00016\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 4.6787e-04 - val_loss: 1.6928e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m983/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.6095e-04  \n",
      "Epoch 4: val_loss improved from 0.00016 to 0.00010, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - loss: 3.6099e-04 - val_loss: 9.7200e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.3802e-04  \n",
      "Epoch 5: val_loss improved from 0.00010 to 0.00008, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - loss: 3.3801e-04 - val_loss: 7.9696e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 3.5780e-04  \n",
      "Epoch 6: val_loss did not improve from 0.00008\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 3.5776e-04 - val_loss: 1.4242e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.0390e-04  \n",
      "Epoch 7: val_loss did not improve from 0.00008\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 78ms/step - loss: 3.0390e-04 - val_loss: 2.0911e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 2.6324e-04  \n",
      "Epoch 8: val_loss did not improve from 0.00008\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 2.6326e-04 - val_loss: 2.0268e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 2.6817e-04  \n",
      "Epoch 9: val_loss improved from 0.00008 to 0.00007, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - loss: 2.6818e-04 - val_loss: 6.8989e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 2.9888e-04  \n",
      "Epoch 10: val_loss improved from 0.00007 to 0.00007, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - loss: 2.9888e-04 - val_loss: 6.5657e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 2.6145e-04  \n",
      "Epoch 11: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 78ms/step - loss: 2.6145e-04 - val_loss: 9.6925e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m983/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 2.7708e-04  \n",
      "Epoch 12: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 2.7707e-04 - val_loss: 1.3437e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 2.9337e-04  \n",
      "Epoch 13: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 76ms/step - loss: 2.9335e-04 - val_loss: 8.0829e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 2.7368e-04  \n",
      "Epoch 14: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 76ms/step - loss: 2.7369e-04 - val_loss: 1.0619e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 2.5468e-04  \n",
      "Epoch 15: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 76ms/step - loss: 2.5469e-04 - val_loss: 6.7029e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 3.2128e-04  \n",
      "Epoch 16: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 76ms/step - loss: 3.2122e-04 - val_loss: 9.9050e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 3.3898e-04  \n",
      "Epoch 17: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 3.3891e-04 - val_loss: 9.1067e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 2.5491e-04  \n",
      "Epoch 18: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 2.5491e-04 - val_loss: 7.3255e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 2.5298e-04  \n",
      "Epoch 19: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 77ms/step - loss: 2.5298e-04 - val_loss: 1.1783e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 3.0249e-04  \n",
      "Epoch 20: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 3.0245e-04 - val_loss: 1.2241e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 2.4236e-04  \n",
      "Epoch 21: val_loss did not improve from 0.00007\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 77ms/step - loss: 2.4237e-04 - val_loss: 8.7121e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 3.6768e-04  \n",
      "Epoch 22: val_loss improved from 0.00007 to 0.00006, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 75ms/step - loss: 3.6757e-04 - val_loss: 6.3085e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.3692e-04  \n",
      "Epoch 23: val_loss did not improve from 0.00006\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.3695e-04 - val_loss: 2.6565e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.4176e-04 \n",
      "Epoch 24: val_loss improved from 0.00006 to 0.00006, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.4176e-04 - val_loss: 5.6515e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.5807e-04 \n",
      "Epoch 25: val_loss did not improve from 0.00006\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 72ms/step - loss: 2.5808e-04 - val_loss: 1.2146e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.3192e-04 \n",
      "Epoch 26: val_loss did not improve from 0.00006\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 71ms/step - loss: 2.3194e-04 - val_loss: 5.9417e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m983/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.3328e-04  \n",
      "Epoch 27: val_loss did not improve from 0.00006\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 70ms/step - loss: 2.3332e-04 - val_loss: 1.2852e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.4985e-04  \n",
      "Epoch 28: val_loss improved from 0.00006 to 0.00006, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.4985e-04 - val_loss: 5.5143e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.4205e-04  \n",
      "Epoch 29: val_loss did not improve from 0.00006\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.4206e-04 - val_loss: 7.3348e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.0623e-04  \n",
      "Epoch 30: val_loss improved from 0.00006 to 0.00005, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.0627e-04 - val_loss: 5.2713e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.7222e-04  \n",
      "Epoch 31: val_loss improved from 0.00005 to 0.00005, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 73ms/step - loss: 2.7220e-04 - val_loss: 5.2019e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.6704e-04  \n",
      "Epoch 32: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.6702e-04 - val_loss: 5.9698e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.2794e-04  \n",
      "Epoch 33: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.2796e-04 - val_loss: 1.1087e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m983/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.3019e-04 \n",
      "Epoch 34: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 72ms/step - loss: 2.3022e-04 - val_loss: 1.1455e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 2.2792e-04  \n",
      "Epoch 35: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 73ms/step - loss: 2.2794e-04 - val_loss: 5.8086e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.5801e-04 \n",
      "Epoch 36: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.5799e-04 - val_loss: 5.3926e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 2.2170e-04  \n",
      "Epoch 37: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 70ms/step - loss: 2.2172e-04 - val_loss: 5.7069e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.3753e-04  \n",
      "Epoch 38: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.3753e-04 - val_loss: 1.0915e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 3.6620e-04 \n",
      "Epoch 39: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 3.6607e-04 - val_loss: 2.0122e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.9642e-04  \n",
      "Epoch 40: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.9636e-04 - val_loss: 6.1786e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.2450e-04 \n",
      "Epoch 41: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.2451e-04 - val_loss: 7.0873e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.1888e-04  \n",
      "Epoch 42: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 73ms/step - loss: 2.1890e-04 - val_loss: 6.4568e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.6975e-04  \n",
      "Epoch 43: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.6971e-04 - val_loss: 6.0769e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m983/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.5798e-04  \n",
      "Epoch 44: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 70ms/step - loss: 2.5795e-04 - val_loss: 6.8437e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.4450e-04  \n",
      "Epoch 45: val_loss improved from 0.00005 to 0.00005, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.4449e-04 - val_loss: 4.8574e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 3.0777e-04 \n",
      "Epoch 46: val_loss improved from 0.00005 to 0.00005, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 3.0770e-04 - val_loss: 4.8125e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 2.5299e-04  \n",
      "Epoch 47: val_loss improved from 0.00005 to 0.00005, saving model to checkpoints/shib_model.keras\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 69ms/step - loss: 2.5297e-04 - val_loss: 4.8098e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.3103e-04 \n",
      "Epoch 48: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 72ms/step - loss: 2.3103e-04 - val_loss: 5.6081e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 2.5208e-04 \n",
      "Epoch 49: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 70ms/step - loss: 2.5206e-04 - val_loss: 6.7304e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 2.0379e-04 \n",
      "Epoch 50: val_loss did not improve from 0.00005\n",
      "\u001b[1m984/984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 71ms/step - loss: 2.0382e-04 - val_loss: 5.6509e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-01 21:18:08,192 - INFO - Model training complete. Checkpoint saved to checkpoints/shib_model.keras.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime, timedelta\n",
    "from coinbase.rest import RESTClient\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Initialize Coinbase client\n",
    "api_key = \"organizations/34ea76db-0149-4b56-a708-2610a3c60ab2/apiKeys/5533fcd1-0647-4784-a9b5-902bf53691c4\"\n",
    "api_secret = \"-----BEGIN EC PRIVATE KEY-----\\nMHcCAQEEIHshL3W7qRla2s+0t0ttn5u8ys70j3eJTwpk50uYCIQ6oAoGCCqGSM49\\nAwEHoUQDQgAEj2YiJKTFGzUeSgcDVpgaAuBV4N9//6ZJ+Li0GdPaBo8Hd0yGQTLc\\nWPGHUaot5a6HklujIN/zt21EgpBERDgl2Q==\\n-----END EC PRIVATE KEY-----\\n\"\n",
    "\n",
    "client = RESTClient(api_key=api_key, api_secret=api_secret)\n",
    "\n",
    "# Parameters\n",
    "trade_coin = \"SHIB-USD\"\n",
    "lookback_window = 30\n",
    "checkpoint_file = \"checkpoints/shib_model.keras\"\n",
    "granularity = 900  # 15-minute candles\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fetch historical data\n",
    "def get_product_candles_all(product_id, start, end, granularity):\n",
    "    logging.info(f\"Fetching historical data for {product_id} from {start} to {end} with granularity {granularity}.\")\n",
    "    url = f\"https://api.exchange.coinbase.com/products/{product_id}/candles\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    all_candles = []\n",
    "    \n",
    "    current_start = datetime.fromisoformat(start)\n",
    "    current_end = current_start + timedelta(seconds=granularity * 300)  # 300 data points max\n",
    "\n",
    "    while current_start < datetime.fromisoformat(end):\n",
    "        # Ensure current_end does not exceed the requested end time\n",
    "        if current_end > datetime.fromisoformat(end):\n",
    "            current_end = datetime.fromisoformat(end)\n",
    "        \n",
    "        params = {\"start\": current_start.isoformat(), \"end\": current_end.isoformat(), \"granularity\": granularity}\n",
    "        try:\n",
    "            logging.info(f\"Requesting data from {current_start.isoformat()} to {current_end.isoformat()}.\")\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            candles = response.json()\n",
    "            if not candles:\n",
    "                logging.warning(f\"No data returned for {current_start.isoformat()} to {current_end.isoformat()}.\")\n",
    "            all_candles.extend(candles)\n",
    "\n",
    "            current_start = current_end\n",
    "            current_end = current_start + timedelta(seconds=granularity * 300)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Error fetching product candles: {e}\")\n",
    "            break\n",
    "\n",
    "    if not all_candles:\n",
    "        logging.error(\"No data fetched. Exiting historical data fetch.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_candles, columns=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\")\n",
    "    df.set_index(\"time\", inplace=True)\n",
    "    logging.info(f\"Historical data fetch complete. Total rows fetched: {len(df)}.\")\n",
    "    return df\n",
    "\n",
    "# Feature engineering\n",
    "def feature_engineering(df):\n",
    "    logging.info(\"Starting feature engineering.\")\n",
    "    df[\"returns\"] = df[\"close\"].pct_change()\n",
    "    df[\"volatility\"] = df[\"close\"].rolling(window=lookback_window).std()\n",
    "    df[\"moving_avg\"] = df[\"close\"].rolling(window=lookback_window).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    logging.info(f\"Feature engineering complete. Final dataset shape: {df.shape}.\")\n",
    "    return df\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def prepare_data(df):\n",
    "    logging.info(\"Preparing data for LSTM.\")\n",
    "    scaled_data = scaler.fit_transform(df.values)\n",
    "    x, y = [], []\n",
    "    for i in range(lookback_window, len(scaled_data)):\n",
    "        x.append(scaled_data[i - lookback_window:i])\n",
    "        y.append(scaled_data[i, 0])\n",
    "    logging.info(f\"Data preparation complete. Total samples: {len(x)}.\")\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(input_shape, units=50, dropout=0.2):\n",
    "    logging.info(f\"Building LSTM model with input shape {input_shape}, units={units}, dropout={dropout}.\")\n",
    "    model = Sequential([\n",
    "        LSTM(units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout),\n",
    "        LSTM(units, return_sequences=False),\n",
    "        Dropout(dropout),\n",
    "        Dense(1, activation=\"linear\")\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss=\"mean_squared_error\")\n",
    "    logging.info(\"LSTM model build complete.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    logging.info(\"Starting hyperparameter tuning with Optuna.\")\n",
    "    units = trial.suggest_int(\"units\", 30, 100)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = build_lstm_model((x_train.shape[1], x_train.shape[2]), units=units, dropout=dropout)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=\"mean_squared_error\")\n",
    "    model.fit(x_train, y_train, epochs=5, batch_size=32, verbose=0)\n",
    "\n",
    "    val_loss = model.evaluate(x_val, y_val, verbose=0)\n",
    "    logging.info(f\"Validation loss: {val_loss}.\")\n",
    "    return val_loss\n",
    "\n",
    "# Training pipeline\n",
    "def train_model():\n",
    "    logging.info(\"Starting model training pipeline.\")\n",
    "    end_time = datetime.utcnow()\n",
    "    start_time = end_time - timedelta(days=365)\n",
    "    df = get_product_candles_all(trade_coin, start_time.isoformat(), end_time.isoformat(), granularity)\n",
    "    if df.empty:\n",
    "        logging.error(\"No historical data available. Exiting training.\")\n",
    "        return\n",
    "\n",
    "    df = feature_engineering(df)\n",
    "    global x, y\n",
    "    x, y = prepare_data(df)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    best_params = study.best_params\n",
    "    logging.info(f\"Best parameters: {best_params}.\")\n",
    "\n",
    "    # Train final model\n",
    "    model = build_lstm_model((x.shape[1], x.shape[2]), units=best_params[\"units\"], dropout=best_params[\"dropout\"])\n",
    "    checkpoint = ModelCheckpoint(filepath=checkpoint_file, save_best_only=True, save_weights_only=False, verbose=1)\n",
    "    model.fit(x, y, epochs=50, batch_size=32, validation_split=0.1, callbacks=[checkpoint], verbose=1)\n",
    "    logging.info(f\"Model training complete. Checkpoint saved to {checkpoint_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d7b2e9-91ef-4424-8291-dbf055b14db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "try:\n",
    "    model = load_model(\"checkpoints/shib_model.keras\")\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bdc3a-7613-41ea-b52a-cb64e90f4d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
